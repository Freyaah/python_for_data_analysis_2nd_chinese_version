{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Freyaah/python_for_data_analysis_2nd_chinese_version/blob/master/Summarization_Rouge_Evaluation_24Feb2023.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6aa8877e",
      "metadata": {
        "id": "6aa8877e"
      },
      "source": [
        "# Import Libaries"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "faabd5a9",
      "metadata": {
        "id": "faabd5a9"
      },
      "source": [
        "useful links\n",
        "\n",
        "https://huggingface.co/transformers/v3.0.2/model_doc/bert.html\n",
        "\n",
        "explanation on rouge differences for extractive vs abstractive summarization\n",
        "\n",
        "https://stats.stackexchange.com/questions/451419/rouge-scores-for-extractive-vs-abstractive-text-summarization\n",
        "\n",
        "website about abstractive summarization\n",
        "\n",
        "https://medium.com/sciforce/towards-automatic-summarization-part-2-abstractive-methods-c424386a65ea\n",
        "\n",
        "BigBirdPegasus: can accomodate up to 4096 vs 512 \n",
        "\n",
        "https://huggingface.co/google/bigbird-pegasus-large-pubmed\n",
        "\n",
        "***Potential models you may want to try can be found:***\n",
        "\n",
        "***https://paperswithcode.com/sota/text-summarization-on-pubmed-1***\n",
        "\n",
        "article that we are using to create the reference summary for history of presenting illness\n",
        "\n",
        "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC9285173/"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#! pip install bert-extractive-summarizer\n",
        "#! pip install evaluate\n",
        "! pip install pysummarization"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oiD6QKbNbFim",
        "outputId": "f5b6002d-c1a1-4b9e-e951-28394359b5b0"
      },
      "id": "oiD6QKbNbFim",
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pysummarization\n",
            "  Downloading pysummarization-1.1.9.tar.gz (64 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.2/64.2 KB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from pysummarization) (1.22.4)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.8/dist-packages (from pysummarization) (3.7)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (from nltk->pysummarization) (4.64.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.8/dist-packages (from nltk->pysummarization) (1.2.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.8/dist-packages (from nltk->pysummarization) (8.1.3)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.8/dist-packages (from nltk->pysummarization) (2022.6.2)\n",
            "Building wheels for collected packages: pysummarization\n",
            "  Building wheel for pysummarization (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pysummarization: filename=pysummarization-1.1.9-py3-none-any.whl size=82376 sha256=92f8fde4d4877ec5226dd2f2166cbe624ef32584282ddc5640e19ded29c44347\n",
            "  Stored in directory: /root/.cache/pip/wheels/b5/28/29/9bad07ea24d08ecc60fc05419196e0e2506dc1e0eb121b0d31\n",
            "Successfully built pysummarization\n",
            "Installing collected packages: pysummarization\n",
            "Successfully installed pysummarization-1.1.9\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "bccc0230",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 375
        },
        "id": "bccc0230",
        "outputId": "6a48ef78-fabf-4e59-a161-873bed5598ce"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-5dce0d0ff5ab>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAutoTokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLongT5ForConditionalGeneration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mpysummarization\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlpbase\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_abstractor\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAutoAbstractor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpysummarization\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenizabledoc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msimple_tokenizer\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSimpleTokenizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpysummarization\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabstractabledoc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtop_n_rank_abstractor\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTopNRankAbstractor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pysummarization'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "import matplotlib\n",
        "\n",
        "# ensure you have the summarizer package installed\n",
        "# ! pip install bert-extractive-summarizer\n",
        "\n",
        "from summarizer import Summarizer\n",
        "\n",
        "from transformers import BertTokenizer, GPT2Tokenizer, EncoderDecoderModel\n",
        "from transformers import AutoTokenizer, AutoModel, AutoModelWithLMHead\n",
        "\n",
        "# use t5 in tf\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
        "\n",
        "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
        "\n",
        "import evaluate\n",
        "from datasets import load_dataset\n",
        "from transformers import AutoTokenizer, LongT5ForConditionalGeneration\n",
        "\n",
        "from pysummarization.nlpbase.auto_abstractor import AutoAbstractor\n",
        "from pysummarization.tokenizabledoc.simple_tokenizer import SimpleTokenizer\n",
        "from pysummarization.abstractabledoc.top_n_rank_abstractor import TopNRankAbstractor\n",
        "import regex as re\n",
        "\n",
        "## for bart\n",
        "import transformers  #(3.0.1)\n",
        "\n",
        "import gensim\n",
        "from gensim.summarization import summarize\n",
        "\n",
        "# to use rouge you need to install it\n",
        "# ! pip install rouge\n",
        "from rouge import Rouge "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "94063234",
      "metadata": {
        "id": "94063234"
      },
      "outputs": [],
      "source": [
        "# if using GPU, this is to check if the cuda installation is properly set up for pytorch\n",
        "import torch\n",
        "print(torch.cuda.is_available())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7b626ee1",
      "metadata": {
        "id": "7b626ee1"
      },
      "outputs": [],
      "source": [
        "import gc\n",
        "##del model when no memory, can use this instead of restarting jupyter.\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "002ad272",
      "metadata": {
        "id": "002ad272"
      },
      "source": [
        "# Import Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "43548bea",
      "metadata": {
        "id": "43548bea"
      },
      "outputs": [],
      "source": [
        "hpi_df = pd.read_csv('./datasets/cohort_hpi_df.csv',index_col=0)\n",
        "hpi_df.shape\n",
        "# #taking a small subset of 10 examples\n",
        "# shpi=hpi_df.iloc[0:10,:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bcdbfc7e",
      "metadata": {
        "id": "bcdbfc7e"
      },
      "outputs": [],
      "source": [
        "shpi= pd.read_csv('./datasets/shpi.csv')\n",
        "shpi.head(2)\n",
        "# shpi=shpi.iloc[0:10,:]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "02570491",
      "metadata": {
        "id": "02570491"
      },
      "source": [
        "# BERT Summarizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b69b4cb1",
      "metadata": {
        "scrolled": true,
        "id": "b69b4cb1"
      },
      "outputs": [],
      "source": [
        "from summarizer import Summarizer\n",
        "\n",
        "model = Summarizer()\n",
        "\n",
        "def BertSummarizer(text):\n",
        "    result = model(text,ratio=0.2)\n",
        "    \n",
        "    return result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d268b977",
      "metadata": {
        "scrolled": true,
        "id": "d268b977"
      },
      "outputs": [],
      "source": [
        "shpi['BertSummarizer'] = shpi['hpi_input_text'].apply(BertSummarizer)\n",
        "shpi['rougeBertSummarizer'] = shpi.apply(lambda x: calculate_rouge(x['BertSummarizer'], x['hpi_reference_summary']), axis=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fc833414",
      "metadata": {
        "id": "fc833414"
      },
      "source": [
        "# BERTGPT2 Model using bioclinical tokenizer"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c66c6f46",
      "metadata": {
        "id": "c66c6f46"
      },
      "source": [
        "paper regarding Bioclinical BERT\n",
        "https://arxiv.org/abs/1904.03323"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "09370491",
      "metadata": {
        "id": "09370491"
      },
      "outputs": [],
      "source": [
        "# tokenizer for bioclinical bert\n",
        "bioclinberttokenizer = AutoTokenizer.from_pretrained(\"emilyalsentzer/Bio_ClinicalBERT\")\n",
        "\n",
        "model = EncoderDecoderModel.from_pretrained(\"patrickvonplaten/bert2gpt2-cnn_dailymail-fp16\")\n",
        "model.to(\"cuda\")\n",
        "\n",
        "#bert_tokenizer = BertTokenizer.from_pretrained(\"bert-base-cased\")\n",
        "bert_tokenizer= AutoTokenizer.from_pretrained(\"emilyalsentzer/Bio_ClinicalBERT\")\n",
        "\n",
        "# CLS token will work as BOS token\n",
        "bert_tokenizer.bos_token = bert_tokenizer.cls_token\n",
        "\n",
        "# SEP token will work as EOS token\n",
        "bert_tokenizer.eos_token = bert_tokenizer.sep_token\n",
        "\n",
        "\n",
        "# make sure GPT2 appends EOS in begin and end\n",
        "def build_inputs_with_special_tokens(self, token_ids_0, token_ids_1=None):\n",
        "    outputs = [self.bos_token_id] + token_ids_0 + [self.eos_token_id]\n",
        "    return outputs\n",
        "\n",
        "\n",
        "GPT2Tokenizer.build_inputs_with_special_tokens = build_inputs_with_special_tokens\n",
        "gpt2_tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
        "# set pad_token_id to unk_token_id -> be careful here as unk_token_id == eos_token_id == bos_token_id\n",
        "gpt2_tokenizer.pad_token = gpt2_tokenizer.unk_token\n",
        "\n",
        "\n",
        "# set decoding params\n",
        "model.config.decoder_start_token_id = gpt2_tokenizer.bos_token_id\n",
        "model.config.eos_token_id = gpt2_tokenizer.eos_token_id\n",
        "model.config.max_length = 142\n",
        "model.config.min_length = 56\n",
        "model.config.no_repeat_ngram_size = 3\n",
        "model.early_stopping = True\n",
        "model.length_penalty = 2.0\n",
        "model.num_beams = 4\n",
        "\n",
        "#test_dataset = nlp.load_dataset(\"cnn_dailymail\", \"3.0.0\", split=\"test\")\n",
        "\n",
        "batch_size = 64"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "21f1efc9",
      "metadata": {
        "id": "21f1efc9"
      },
      "outputs": [],
      "source": [
        "def generate_BERT2GPT2_summary(batch):\n",
        "    # Tokenizer will automatically set [BOS] <text> [EOS]\n",
        "    # cut off at BERT max length 512\n",
        "    inputs = bert_tokenizer(batch, padding=\"max_length\", truncation=True, max_length=512, return_tensors=\"pt\")\n",
        "    input_ids = inputs.input_ids.to(\"cuda\")\n",
        "    attention_mask = inputs.attention_mask.to(\"cuda\")\n",
        "\n",
        "    outputs = model.generate(input_ids, attention_mask=attention_mask)\n",
        "\n",
        "    # all special tokens including will be removed\n",
        "    output_str = gpt2_tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
        "\n",
        "    #batch[\"pred\"] = output_str\n",
        "\n",
        "    return output_str"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8e78a077",
      "metadata": {
        "id": "8e78a077"
      },
      "outputs": [],
      "source": [
        "shpi['BertGPT2'] = shpi['hpi_input_text'].apply(generate_BERT2GPT2_summary)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e6a25654",
      "metadata": {
        "id": "e6a25654"
      },
      "source": [
        "# T5"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e4f6763b",
      "metadata": {
        "id": "e4f6763b"
      },
      "source": [
        "## T5 Seq2Seq"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d11bc1ca",
      "metadata": {
        "id": "d11bc1ca"
      },
      "outputs": [],
      "source": [
        "DEVICE = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(\"t5-base\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"t5-base\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3891ba9b",
      "metadata": {
        "id": "3891ba9b"
      },
      "outputs": [],
      "source": [
        "def seq2seq(text):\n",
        "\n",
        "    inputs = tokenizer(\"summarize: \" + text, return_tensors=\"pt\", max_length=512, truncation=True)\n",
        "    outputs = model.generate(inputs[\"input_ids\"], max_length=150, min_length=40, length_penalty=2.0, num_beams=4, early_stopping=True)\n",
        "\n",
        "    summary= tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    \n",
        "    return summary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0c37abd7",
      "metadata": {
        "id": "0c37abd7"
      },
      "outputs": [],
      "source": [
        "shpi['t5seq2eq'] = shpi['hpi_input_text'].apply(seq2seq)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9342d395",
      "metadata": {
        "id": "9342d395"
      },
      "source": [
        "## T5 Base"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a7dbf776",
      "metadata": {
        "id": "a7dbf776"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "\n",
        "t5_model = T5ForConditionalGeneration.from_pretrained(\"t5-base\").to(device)\n",
        "t5_tokenizer = T5Tokenizer.from_pretrained(\"t5-base\")\n",
        "num_beams=2\n",
        "min_length=15\n",
        "max_length=200\n",
        "\n",
        "model='t5'\n",
        "if model == 't5':\n",
        "    _num_beams = 1\n",
        "    _no_repeat_ngram_size = 1\n",
        "    _length_penalty = 2\n",
        "    _min_length = 30\n",
        "    _max_length = 200\n",
        "    _early_stopping = True\n",
        "\n",
        "def t5(input_text):\n",
        "    input_text = str(input_text).replace('\\n', '')\n",
        "    input_text = ' '.join(input_text.split())\n",
        "    input_tokenized = t5_tokenizer.encode(input_text, return_tensors=\"pt\").to(device)\n",
        "    summary_task = torch.tensor([[21603, 10]]).to(device)\n",
        "    input_tokenized = torch.cat([summary_task, input_tokenized], dim=-1).to(device)\n",
        "    summary_ids = t5_model.generate(input_tokenized,\n",
        "                                        num_beams=_num_beams,\n",
        "                                        no_repeat_ngram_size=_no_repeat_ngram_size,\n",
        "                                        length_penalty=_length_penalty,\n",
        "                                        min_length=_min_length,\n",
        "                                        max_length=_max_length,\n",
        "                                        early_stopping=_early_stopping)\n",
        "    output = [t5_tokenizer.decode(g, skip_special_tokens=True, clean_up_tokenization_spaces=False) for g in summary_ids]\n",
        "    return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "56c23a07",
      "metadata": {
        "id": "56c23a07"
      },
      "outputs": [],
      "source": [
        "shpi['t5'] = shpi['hpi_input_text'].apply(t5)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a0d37d51",
      "metadata": {
        "id": "a0d37d51"
      },
      "source": [
        "## Long T5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "06b9e3ff",
      "metadata": {
        "id": "06b9e3ff"
      },
      "outputs": [],
      "source": [
        "model = (\n",
        "    LongT5ForConditionalGeneration.from_pretrained(\"Stancld/longt5-tglobal-large-16384-pubmed-3k_steps\")\n",
        "    .to(\"cuda\")\n",
        "    .half()\n",
        ")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"Stancld/longt5-tglobal-large-16384-pubmed-3k_steps\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2fe64639",
      "metadata": {
        "id": "2fe64639"
      },
      "outputs": [],
      "source": [
        "def generate_answers_longT5(batch):\n",
        "    inputs_dict = tokenizer(\n",
        "        batch, max_length=500, padding=\"max_length\", truncation=True, return_tensors=\"pt\"\n",
        "    )\n",
        "    input_ids = inputs_dict.input_ids.to(\"cuda\")\n",
        "    attention_mask = inputs_dict.attention_mask.to(\"cuda\")\n",
        "    output_ids = model.generate(input_ids, attention_mask=attention_mask, max_length=200, num_beams=4)\n",
        "    summary = tokenizer.batch_decode(output_ids, skip_special_tokens=True)\n",
        "    return summary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "33b26378",
      "metadata": {
        "id": "33b26378"
      },
      "outputs": [],
      "source": [
        "shpi['longt5'] = shpi['hpi_input_text'].apply(generate_answers_longT5)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4f595f37",
      "metadata": {
        "id": "4f595f37"
      },
      "source": [
        "# PySummarizer"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "218da2aa",
      "metadata": {
        "id": "218da2aa"
      },
      "source": [
        "https://pypi.org/project/pysummarization/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6afd9e88",
      "metadata": {
        "id": "6afd9e88"
      },
      "outputs": [],
      "source": [
        "auto_abstractor = AutoAbstractor()\n",
        "auto_abstractor.tokenizable_doc = SimpleTokenizer()\n",
        "auto_abstractor.delimiter_list = [\".\", \"\\n\"]\n",
        "abstractable_doc = TopNRankAbstractor()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b3368dad",
      "metadata": {
        "id": "b3368dad"
      },
      "outputs": [],
      "source": [
        "shpi['transcription'] = shpi['hpi_input_text'].astype(str)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "69c9dead",
      "metadata": {
        "id": "69c9dead"
      },
      "outputs": [],
      "source": [
        "def pysummarizer(text):\n",
        "#    print(type(text))\n",
        "    summary = auto_abstractor.summarize(text, abstractable_doc)\n",
        "    best_sentences=[]\n",
        "    #summary_clean = ''.join([str(sentence).capitalize() for sentence in summary['summarize_result'] for summary['summarize_result'] in auto_abstractor.summarize(text, abstractable_doc)])\n",
        "    for sentence in summary['summarize_result']:\n",
        "        best_sentences.append(re.sub(r'\\s+', ' ', sentence).strip())    \n",
        "    clean_summary=''.join(sentence for sentence in best_sentences)\n",
        "    return clean_summary\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9173cd33",
      "metadata": {
        "id": "9173cd33"
      },
      "outputs": [],
      "source": [
        "shpi['pysummarizer'] = shpi['transcription'].apply(pysummarizer)\n",
        "shpi = shpi.drop('transcription', axis=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b99ccaa5",
      "metadata": {
        "id": "b99ccaa5"
      },
      "source": [
        "# Bart"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b0fbc244",
      "metadata": {
        "id": "b0fbc244"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "Summarizes corpus with Bart.\n",
        ":parameter    \n",
        "   :param corpus: list - dtf[\"text\"]    \n",
        "   :param max_len: length of the summary\n",
        ":return    \n",
        "    list of summaries\n",
        "'''\n",
        "def bart(corpus, max_len):    \n",
        "    nlp = transformers.pipeline(\"summarization\")    \n",
        "    lst_summaries = [nlp(txt,               \n",
        "                         max_length=max_len\n",
        "                         )[0][\"summary_text\"].replace(\" .\", \".\")                    \n",
        "                     for txt in corpus]    \n",
        "    return lst_summaries\n",
        "\n",
        "## Apply the function to corpus\n",
        "predicted = bart(corpus=shpi['hpi_input_text'], max_len=100)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d6851fb2",
      "metadata": {
        "id": "d6851fb2"
      },
      "source": [
        "# Gensim"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cf582b1c",
      "metadata": {
        "id": "cf582b1c"
      },
      "source": [
        "https://radimrehurek.com/gensim_3.8.3/summarization/summariser.html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "72fde431",
      "metadata": {
        "id": "72fde431"
      },
      "outputs": [],
      "source": [
        "shpi['gensim'] = ''\n",
        "for (idx, row) in shpi.iterrows():\n",
        "#     print(idx)\n",
        "    try:\n",
        "        input_text = shpi['hpi_input_text'][idx]\n",
        "        shpi['gensim'][idx] = summarize(str(input_text))\n",
        "    except:\n",
        "        shpi['gensim'][idx] = row.loc['gensim']\n",
        "        print(row.loc['gensim'],row)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4e873ede",
      "metadata": {
        "id": "4e873ede"
      },
      "source": [
        "# Generate Rouge Score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4afcf9ec",
      "metadata": {
        "id": "4afcf9ec"
      },
      "outputs": [],
      "source": [
        "rouge = Rouge()\n",
        "\n",
        "def calculate_rouge(predicted, reference):\n",
        "\n",
        "    scores = rouge.get_scores(predicted, reference)\n",
        "    return scores"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6be4ad6e",
      "metadata": {
        "id": "6be4ad6e"
      },
      "outputs": [],
      "source": [
        "modelsrouge=['BertSummarizer','BertGPT2','t5seq2eq','t5','gensim','pysummarizer']\n",
        "\n",
        "for model in modelsrouge:\n",
        "    shpi['rouge'+model] = ''\n",
        "    for (idx, row) in shpi.iterrows():\n",
        "        try:\n",
        "            shpi['rouge'+model][idx] = calculate_rouge(shpi[model][idx], shpi['hpi_reference_summary'][idx])\n",
        "        except:\n",
        "            shpi['rouge'+model][idx] = ''   "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "559c26f4",
      "metadata": {
        "id": "559c26f4"
      },
      "outputs": [],
      "source": [
        "for model in modelsrouge:\n",
        "    shpi['r1_recall_'+model] = ''\n",
        "    shpi['r1_precision_'+model] = ''\n",
        "    shpi['r1_f_'+model] = ''\n",
        "    shpi['r2_recall_'+model] = ''\n",
        "    shpi['r2_precision_'+model] = ''\n",
        "    shpi['r2_f_'+model] = ''\n",
        "    shpi['rl_recall_'+model] = ''\n",
        "    shpi['rl_precision_'+model] = ''\n",
        "    shpi['rl_f_'+model] = ''\n",
        "\n",
        "    for (idx, row) in shpi.iterrows():\n",
        "        try: \n",
        "            rs = shpi['rouge'+model][idx][0]\n",
        "            shpi['r1_recall_'+model][idx] = rs['rouge-1']['r']\n",
        "            shpi['r1_precision_'+model][idx] = rs['rouge-1']['p']\n",
        "            shpi['r1_f_'+model][idx] = rs['rouge-1']['f']\n",
        "\n",
        "            shpi['r2_recall_'+model][idx] = rs['rouge-2']['r']\n",
        "            shpi['r2_precision_'+model][idx] = rs['rouge-2']['p']\n",
        "            shpi['r2_f_'+model][idx] = rs['rouge-2']['f']\n",
        "\n",
        "            shpi['rl_recall_'+model][idx] = rs['rouge-l']['r']\n",
        "            shpi['rl_precision_'+model][idx] = rs['rouge-l']['p']\n",
        "            shpi['rl_f_'+model][idx] = rs['rouge-l']['f']\n",
        "        except:\n",
        "            shpi['r1_recall_'+model][idx] = 0\n",
        "            shpi['r1_precision_'+model][idx] = 0\n",
        "            shpi['r1_f_'+model][idx] = 0\n",
        "\n",
        "            shpi['r2_recall_'+model][idx] = 0\n",
        "            shpi['r2_precision_'+model][idx] = 0\n",
        "            shpi['r2_f_'+model][idx] = 0\n",
        "\n",
        "            shpi['rl_recall_'+model][idx] = 0\n",
        "            shpi['rl_precision_'+model][idx] = 0\n",
        "            shpi['rl_f_'+model][idx] = 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "12091354",
      "metadata": {
        "id": "12091354"
      },
      "outputs": [],
      "source": [
        "shpi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "708f063c",
      "metadata": {
        "id": "708f063c"
      },
      "outputs": [],
      "source": [
        "pd.pandas.set_option('display.max_columns', None)\n",
        "shpi.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "906b295b",
      "metadata": {
        "id": "906b295b"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.10"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {
        "height": "calc(100% - 180px)",
        "left": "10px",
        "top": "150px",
        "width": "256px"
      },
      "toc_section_display": true,
      "toc_window_display": true
    },
    "varInspector": {
      "cols": {
        "lenName": 16,
        "lenType": 16,
        "lenVar": 40
      },
      "kernels_config": {
        "python": {
          "delete_cmd_postfix": "",
          "delete_cmd_prefix": "del ",
          "library": "var_list.py",
          "varRefreshCmd": "print(var_dic_list())"
        },
        "r": {
          "delete_cmd_postfix": ") ",
          "delete_cmd_prefix": "rm(",
          "library": "var_list.r",
          "varRefreshCmd": "cat(var_dic_list()) "
        }
      },
      "types_to_exclude": [
        "module",
        "function",
        "builtin_function_or_method",
        "instance",
        "_Feature"
      ],
      "window_display": false
    },
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}